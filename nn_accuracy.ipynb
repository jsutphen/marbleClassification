{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 3.4379, Val Loss: 3.3013, Val Acc: 17.57%\n",
      "Epoch [2/100], Train Loss: 3.1977, Val Loss: 3.0821, Val Acc: 31.76%\n",
      "Epoch [3/100], Train Loss: 3.0051, Val Loss: 2.9029, Val Acc: 29.73%\n",
      "Epoch [4/100], Train Loss: 2.8623, Val Loss: 2.7663, Val Acc: 31.08%\n",
      "Epoch [5/100], Train Loss: 2.7418, Val Loss: 2.6256, Val Acc: 32.43%\n",
      "Epoch [6/100], Train Loss: 2.6586, Val Loss: 2.5451, Val Acc: 32.43%\n",
      "Epoch [7/100], Train Loss: 2.5931, Val Loss: 2.4859, Val Acc: 33.11%\n",
      "Epoch [8/100], Train Loss: 2.5316, Val Loss: 2.4454, Val Acc: 33.78%\n",
      "Epoch [9/100], Train Loss: 2.4945, Val Loss: 2.3870, Val Acc: 34.46%\n",
      "Epoch [10/100], Train Loss: 2.4803, Val Loss: 2.3485, Val Acc: 33.78%\n",
      "Epoch [11/100], Train Loss: 2.4311, Val Loss: 2.3140, Val Acc: 33.78%\n",
      "Epoch [12/100], Train Loss: 2.3971, Val Loss: 2.2827, Val Acc: 33.78%\n",
      "Epoch [13/100], Train Loss: 2.3653, Val Loss: 2.2441, Val Acc: 34.46%\n",
      "Epoch [14/100], Train Loss: 2.3471, Val Loss: 2.2357, Val Acc: 35.81%\n",
      "Epoch [15/100], Train Loss: 2.2929, Val Loss: 2.2045, Val Acc: 37.16%\n",
      "Epoch [16/100], Train Loss: 2.3141, Val Loss: 2.2041, Val Acc: 36.49%\n",
      "Epoch [17/100], Train Loss: 2.2749, Val Loss: 2.1792, Val Acc: 36.49%\n",
      "Epoch [18/100], Train Loss: 2.2808, Val Loss: 2.1699, Val Acc: 40.54%\n",
      "Epoch [19/100], Train Loss: 2.2842, Val Loss: 2.1745, Val Acc: 37.16%\n",
      "Epoch [20/100], Train Loss: 2.2492, Val Loss: 2.1487, Val Acc: 39.19%\n",
      "Epoch [21/100], Train Loss: 2.2536, Val Loss: 2.1344, Val Acc: 37.16%\n",
      "Epoch [22/100], Train Loss: 2.2358, Val Loss: 2.1244, Val Acc: 40.54%\n",
      "Epoch [23/100], Train Loss: 2.2292, Val Loss: 2.1205, Val Acc: 40.54%\n",
      "Epoch [24/100], Train Loss: 2.2283, Val Loss: 2.1114, Val Acc: 37.84%\n",
      "Epoch [25/100], Train Loss: 2.2182, Val Loss: 2.1041, Val Acc: 38.51%\n",
      "Epoch [26/100], Train Loss: 2.2448, Val Loss: 2.1012, Val Acc: 43.24%\n",
      "Epoch [27/100], Train Loss: 2.2062, Val Loss: 2.0979, Val Acc: 42.57%\n",
      "Epoch [28/100], Train Loss: 2.1817, Val Loss: 2.0790, Val Acc: 43.92%\n",
      "Epoch [29/100], Train Loss: 2.1604, Val Loss: 2.0775, Val Acc: 43.92%\n",
      "Epoch [30/100], Train Loss: 2.1883, Val Loss: 2.0838, Val Acc: 45.27%\n",
      "Epoch [31/100], Train Loss: 2.1541, Val Loss: 2.0663, Val Acc: 45.27%\n",
      "Epoch [32/100], Train Loss: 2.1560, Val Loss: 2.0702, Val Acc: 42.57%\n",
      "Epoch [33/100], Train Loss: 2.1702, Val Loss: 2.0628, Val Acc: 43.92%\n",
      "Epoch [34/100], Train Loss: 2.1776, Val Loss: 2.0605, Val Acc: 45.27%\n",
      "Epoch [35/100], Train Loss: 2.1658, Val Loss: 2.0734, Val Acc: 43.92%\n",
      "Epoch [36/100], Train Loss: 2.1677, Val Loss: 2.0801, Val Acc: 43.92%\n",
      "Epoch [37/100], Train Loss: 2.1501, Val Loss: 2.0736, Val Acc: 45.27%\n",
      "Epoch [38/100], Train Loss: 2.1638, Val Loss: 2.0705, Val Acc: 43.24%\n",
      "Epoch [39/100], Train Loss: 2.1672, Val Loss: 2.0737, Val Acc: 43.92%\n",
      "Epoch [40/100], Train Loss: 2.1515, Val Loss: 2.0667, Val Acc: 44.59%\n",
      "Epoch [41/100], Train Loss: 2.1769, Val Loss: 2.0643, Val Acc: 44.59%\n",
      "Epoch [42/100], Train Loss: 2.1576, Val Loss: 2.0534, Val Acc: 43.92%\n",
      "Epoch [43/100], Train Loss: 2.1366, Val Loss: 2.0575, Val Acc: 44.59%\n",
      "Epoch [44/100], Train Loss: 2.1828, Val Loss: 2.0607, Val Acc: 44.59%\n",
      "Epoch [45/100], Train Loss: 2.1425, Val Loss: 2.0485, Val Acc: 45.27%\n",
      "Epoch [46/100], Train Loss: 2.1582, Val Loss: 2.0647, Val Acc: 42.57%\n",
      "Epoch [47/100], Train Loss: 2.1498, Val Loss: 2.0592, Val Acc: 43.24%\n",
      "Epoch [48/100], Train Loss: 2.1714, Val Loss: 2.0569, Val Acc: 43.92%\n",
      "Epoch [49/100], Train Loss: 2.1358, Val Loss: 2.0731, Val Acc: 44.59%\n",
      "Epoch [50/100], Train Loss: 2.1452, Val Loss: 2.0682, Val Acc: 43.24%\n",
      "Epoch [51/100], Train Loss: 2.1644, Val Loss: 2.0583, Val Acc: 44.59%\n",
      "Epoch [52/100], Train Loss: 2.1580, Val Loss: 2.0685, Val Acc: 43.92%\n",
      "Epoch [53/100], Train Loss: 2.1832, Val Loss: 2.0546, Val Acc: 43.24%\n",
      "Epoch [54/100], Train Loss: 2.1456, Val Loss: 2.0488, Val Acc: 43.92%\n",
      "Epoch [55/100], Train Loss: 2.1361, Val Loss: 2.0679, Val Acc: 43.92%\n",
      "Epoch [56/100], Train Loss: 2.1702, Val Loss: 2.0493, Val Acc: 43.24%\n",
      "Epoch [57/100], Train Loss: 2.1779, Val Loss: 2.0617, Val Acc: 43.92%\n",
      "Epoch [58/100], Train Loss: 2.1308, Val Loss: 2.0561, Val Acc: 43.24%\n",
      "Epoch [59/100], Train Loss: 2.1239, Val Loss: 2.0653, Val Acc: 43.92%\n",
      "Epoch [60/100], Train Loss: 2.1779, Val Loss: 2.0533, Val Acc: 43.24%\n",
      "Epoch [61/100], Train Loss: 2.1439, Val Loss: 2.0542, Val Acc: 43.24%\n",
      "Epoch [62/100], Train Loss: 2.1554, Val Loss: 2.0455, Val Acc: 43.92%\n",
      "Epoch [63/100], Train Loss: 2.1427, Val Loss: 2.0644, Val Acc: 43.92%\n",
      "Epoch [64/100], Train Loss: 2.1680, Val Loss: 2.0602, Val Acc: 39.86%\n",
      "Epoch [65/100], Train Loss: 2.1447, Val Loss: 2.0486, Val Acc: 43.24%\n",
      "Epoch [66/100], Train Loss: 2.1592, Val Loss: 2.0629, Val Acc: 44.59%\n",
      "Epoch [67/100], Train Loss: 2.1542, Val Loss: 2.0533, Val Acc: 45.27%\n",
      "Epoch [68/100], Train Loss: 2.1568, Val Loss: 2.0510, Val Acc: 44.59%\n",
      "Epoch [69/100], Train Loss: 2.1595, Val Loss: 2.0539, Val Acc: 44.59%\n",
      "Epoch [70/100], Train Loss: 2.1479, Val Loss: 2.0561, Val Acc: 43.92%\n",
      "Epoch [71/100], Train Loss: 2.1336, Val Loss: 2.0517, Val Acc: 43.92%\n",
      "Epoch [72/100], Train Loss: 2.1472, Val Loss: 2.0437, Val Acc: 43.24%\n",
      "Epoch [73/100], Train Loss: 2.1735, Val Loss: 2.0471, Val Acc: 43.24%\n",
      "Epoch [74/100], Train Loss: 2.1095, Val Loss: 2.0499, Val Acc: 44.59%\n",
      "Epoch [75/100], Train Loss: 2.1697, Val Loss: 2.0573, Val Acc: 44.59%\n",
      "Epoch [76/100], Train Loss: 2.1712, Val Loss: 2.0490, Val Acc: 43.24%\n",
      "Epoch [77/100], Train Loss: 2.1412, Val Loss: 2.0591, Val Acc: 44.59%\n",
      "Epoch [78/100], Train Loss: 2.1576, Val Loss: 2.0520, Val Acc: 45.27%\n",
      "Epoch [79/100], Train Loss: 2.1279, Val Loss: 2.0579, Val Acc: 42.57%\n",
      "Epoch [80/100], Train Loss: 2.1342, Val Loss: 2.0447, Val Acc: 43.92%\n",
      "Epoch [81/100], Train Loss: 2.1630, Val Loss: 2.0782, Val Acc: 41.22%\n",
      "Epoch [82/100], Train Loss: 2.1498, Val Loss: 2.0467, Val Acc: 43.92%\n",
      "Epoch [83/100], Train Loss: 2.1179, Val Loss: 2.0464, Val Acc: 43.92%\n",
      "Epoch [84/100], Train Loss: 2.1764, Val Loss: 2.0543, Val Acc: 45.27%\n",
      "Epoch [85/100], Train Loss: 2.1520, Val Loss: 2.0558, Val Acc: 43.24%\n",
      "Epoch [86/100], Train Loss: 2.1287, Val Loss: 2.0525, Val Acc: 45.27%\n",
      "Epoch [87/100], Train Loss: 2.1300, Val Loss: 2.0460, Val Acc: 44.59%\n",
      "Epoch [88/100], Train Loss: 2.1374, Val Loss: 2.0529, Val Acc: 44.59%\n",
      "Epoch [89/100], Train Loss: 2.1551, Val Loss: 2.0483, Val Acc: 44.59%\n",
      "Epoch [90/100], Train Loss: 2.1802, Val Loss: 2.0438, Val Acc: 44.59%\n",
      "Epoch [91/100], Train Loss: 2.1603, Val Loss: 2.0545, Val Acc: 45.27%\n",
      "Epoch [92/100], Train Loss: 2.1717, Val Loss: 2.0563, Val Acc: 43.92%\n",
      "Epoch [93/100], Train Loss: 2.1589, Val Loss: 2.0637, Val Acc: 45.27%\n",
      "Epoch [94/100], Train Loss: 2.1844, Val Loss: 2.0439, Val Acc: 43.92%\n",
      "Epoch [95/100], Train Loss: 2.1643, Val Loss: 2.0530, Val Acc: 43.92%\n",
      "Epoch [96/100], Train Loss: 2.1587, Val Loss: 2.0458, Val Acc: 43.92%\n",
      "Epoch [97/100], Train Loss: 2.1650, Val Loss: 2.0548, Val Acc: 44.59%\n",
      "Epoch [98/100], Train Loss: 2.1479, Val Loss: 2.0488, Val Acc: 44.59%\n",
      "Epoch [99/100], Train Loss: 2.1582, Val Loss: 2.0504, Val Acc: 44.59%\n",
      "Epoch [100/100], Train Loss: 2.1354, Val Loss: 2.0592, Val Acc: 43.92%\n",
      "\n",
      "Final Test Top-1 Accuracy: 39.57%\n",
      "Final Test Top-3 Accuracy: 67.48%\n",
      "d18O=-2.63, d13C=3.44 => Pro\n",
      "d18O=-1.53, d13C=3.83 => Pro\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import random\n",
    "\n",
    "# ----- 1. Set Random Seeds for Reproducibility -----\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# ----- 2. Load Data -----\n",
    "df = pd.read_csv('dataset.csv')\n",
    "# Drop rows where critical columns are missing\n",
    "df = df.dropna(subset=['d18O', 'd13C', 'MARBLE GROUP'])\n",
    "\n",
    "# ----- 3. Encode Labels -----\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label'] = label_encoder.fit_transform(df['MARBLE GROUP'])\n",
    "\n",
    "# Prepare features\n",
    "features = ['d18O', 'd13C']\n",
    "X = df[features].values\n",
    "y = df['Label'].values\n",
    "\n",
    "# ----- 4. Train / Test Split -----\n",
    "# We create an 80/20 train/test split.\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "# Further split the training set into train/validation, e.g. 90/10\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.1, random_state=SEED, stratify=y_train_full\n",
    ")\n",
    "\n",
    "# ----- 5. Scale Features -----\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ----- 6. Create PyTorch Datasets -----\n",
    "class MarbleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = MarbleDataset(X_train_scaled, y_train)\n",
    "val_dataset   = MarbleDataset(X_val_scaled,   y_val)\n",
    "test_dataset  = MarbleDataset(X_test_scaled,  y_test)\n",
    "\n",
    "# ----- 7. Dataloaders -----\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ----- 8. Define Model with BatchNorm & Dropout -----\n",
    "class MarbleNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_size=16, dropout=0.2):\n",
    "        super(MarbleNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "model = MarbleNet(input_dim, num_classes, hidden_size=16, dropout=0.2)\n",
    "\n",
    "# ----- 9. Training Setup -----\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)  # reduce LR every 30 epochs by 10x\n",
    "\n",
    "# ----- 10. Training Loop with Early Stopping -----\n",
    "def evaluate(dataloader):\n",
    "    \"\"\"Compute the average loss and accuracy for a given dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            total_loss += loss.item() * batch_y.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total_samples += batch_y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_acc = 0.0\n",
    "patience = 100 # stop if no improvement for these many epochs\n",
    "counter_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_train_samples = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * batch_y.size(0)\n",
    "        total_train_samples += batch_y.size(0)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Compute training metrics\n",
    "    train_loss = running_loss / total_train_samples\n",
    "    \n",
    "    # Compute validation metrics\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        counter_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # save best model\n",
    "    else:\n",
    "        counter_no_improve += 1\n",
    "        if counter_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# ----- 11. Load the Best Model -----\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# ----- 12. Final Evaluation on Test Set (Top-1 & Top-3) -----\n",
    "model.eval()\n",
    "correct_top1, correct_top3 = 0, 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        # --- Top-1 Accuracy ---\n",
    "        _, predicted_top1 = torch.max(outputs, 1)\n",
    "        correct_top1 += (predicted_top1 == batch_y).sum().item()\n",
    "        \n",
    "        # --- Top-3 Accuracy ---\n",
    "        # Get the indices of the top 3 probabilities for each sample\n",
    "        _, top3_indices = torch.topk(outputs, 3, dim=1)\n",
    "        for i in range(batch_y.size(0)):\n",
    "            if batch_y[i] in top3_indices[i]:\n",
    "                correct_top3 += 1\n",
    "        \n",
    "        total += batch_y.size(0)\n",
    "\n",
    "top1_accuracy = 100.0 * correct_top1 / total\n",
    "top3_accuracy = 100.0 * correct_top3 / total\n",
    "\n",
    "print(f\"\\nFinal Test Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
    "print(f\"Final Test Top-3 Accuracy: {top3_accuracy:.2f}%\")\n",
    "\n",
    "# ----- 13. Predict on New Samples -----\n",
    "new_samples = np.array([\n",
    "    [-2.63, 3.44],\n",
    "    [-1.53, 3.83]\n",
    "])\n",
    "new_samples_scaled = scaler.transform(new_samples)\n",
    "new_tensor = torch.tensor(new_samples_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(new_tensor)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "predicted_labels = label_encoder.inverse_transform(preds.cpu().numpy())\n",
    "for sample, label in zip(new_samples, predicted_labels):\n",
    "    print(f\"d18O={sample[0]}, d13C={sample[1]} => {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
